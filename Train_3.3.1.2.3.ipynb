{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superguine/Object_Detection_Project/blob/main/Train_3.3.1.2.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object Detection from Images Using TensorFlow: A Deep Learning Approach\n",
        "\n",
        "#### Final Year Project Notebook  \n",
        "**B.Tech – Department of Computer Science & Engineering (CSE) & Information Technology (IT)**  \n",
        "**Bankura Unnayani Institute of Engineering (BUIE)**  \n",
        "\n",
        "---\n",
        "\n",
        "### Project Members\n",
        "- Shawon Roy  \n",
        "- Ishika Lohar  \n",
        "- Shreya Dey  \n",
        "- Tiasa Das  \n",
        "\n",
        "### Project Supervisor  \n",
        "Mr. Amit Goswami  \n",
        "Assistant Professor  \n",
        "\n",
        "### Notebook Prepared By  \n",
        "Shawon Roy\n",
        "\n",
        "---\n",
        "\n",
        "> **Disclaimer:**  \n",
        "> _While the overall structure and most of the code in this notebook have been developed by me (Shawon Roy), a few sections (specifically 2–4 key code cells) have been adapted or reused from open-source projects and contributions by the TensorFlow developer community and other ML developers._  \n",
        ">\n",
        "> _Given the complexity of setting up a complete object detection pipeline, these integrations were essential to meet the project goals efficiently and accurately._  \n",
        "> _Full credit and acknowledgment go to the respective developers and original sources._\n",
        "\n",
        "**Last Updated:** May 24, 2025  \n",
        "**Changelog:** [View on GitHub](https://github.com/superguine/ODProject/blob/main/Changelog.md)\n"
      ],
      "metadata": {
        "id": "ktkJE5_JVd9g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UgSOSgCMFyn"
      },
      "source": [
        "# Installation Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in7fD3qqMPO3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Checks for GPU and lists them.\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Sk3yQJwMW7_"
      },
      "outputs": [],
      "source": [
        "# Clone the tensorflow models repository from GitHub\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQwBzGT4MW5P"
      },
      "outputs": [],
      "source": [
        "# Copy setup files into models/research folder.\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EldPCYnoMWym"
      },
      "outputs": [],
      "source": [
        "# Modify setup.py file to install the tf-models-official repository targeted at TF v2.15.0\n",
        "import re\n",
        "with open('/content/models/research/object_detection/packages/tf2/setup.py') as f:\n",
        "    s = f.read()\n",
        "\n",
        "with open('/content/models/research/setup.py', 'w') as f:\n",
        "    # Set fine_tune_checkpoint path\n",
        "    s = re.sub('tf-models-official>=2.5.1',\n",
        "               'tf-models-official==2.15.0', s)\n",
        "    f.write(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️**NOTE :**  \n",
        "Below cell will ask for a session restart. Make sure that the cell completes execution before you restart the session."
      ],
      "metadata": {
        "id": "rjmiEO1VkmeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSnVwDpitEvp"
      },
      "outputs": [],
      "source": [
        "# Need to do a temporary fix with PyYAML because Colab isn't able to install PyYAML v5.4.1\n",
        "!pip install pyyaml==5.3.1\n",
        "!pip install /content/models/research/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below cell will take 10-15 minutes to complete execution."
      ],
      "metadata": {
        "id": "zHfiGNpuoIW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlgbDD38MWtr"
      },
      "outputs": [],
      "source": [
        "# Uninstall existing TensorFlow\n",
        "!pip uninstall -y tensorflow tensorflow-estimator tensorboard\n",
        "\n",
        "# Install NVIDIA CUDA Toolkit 11.8 (needed for TensorFlow 2.15.0)\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
        "!sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu2204-11-8-local_11.8.0-520.61.05-1_amd64.deb\n",
        "!sudo cp /var/cuda-repo-ubuntu2204-11-8-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y cuda-toolkit-11-8\n",
        "\n",
        "# Install cuDNN 8.6 (needed for TensorFlow 2.15.0)\n",
        "!wget https://developer.download.nvidia.com/compute/redist/cudnn/v8.6.0/local_installers/cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz\n",
        "!tar -xvf cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz\n",
        "!sudo cp -r cudnn-linux-x86_64-8.6.0.163_cuda11-archive/include/* /usr/local/cuda/include/\n",
        "!sudo cp -r cudnn-linux-x86_64-8.6.0.163_cuda11-archive/lib/* /usr/local/cuda/lib64/\n",
        "!sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-11.8'\n",
        "os.environ['PATH'] += ':/usr/local/cuda-11.8/bin'\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.8/lib64:' + os.environ.get('LD_LIBRARY_PATH', '')\n",
        "\n",
        "# Install TensorFlow 2.15.0 (GPU Version)\n",
        "!pip install tensorflow[and-cuda]==2.15.0 --extra-index-url https://pypi.nvidia.com\n",
        "\n",
        "# Verify GPU Availability\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOZR2DmNXvZk"
      },
      "outputs": [],
      "source": [
        "# Optional - just to see if everything is good till now.\n",
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️NOTE :\n",
        "Below cell will ask for a session restart. Make sure that the cell completes execution before you restart the session. If it doesn't, asks automatically, you have to manually click the [**RESTART SESSION**] button appears in the output section."
      ],
      "metadata": {
        "id": "LBONomorqEL1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JMSSibXmhLU"
      },
      "outputs": [],
      "source": [
        "#Uninstall & reinstall compatible protobuf version.\n",
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==3.20.3\n",
        "\n",
        "# Set environment type.\n",
        "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QmkwR44NUAl"
      },
      "source": [
        "#### Test installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX-spWpqMWps"
      },
      "outputs": [],
      "source": [
        "# This script tests all of the installations.\n",
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aki7F4pHrBZK"
      },
      "outputs": [],
      "source": [
        "# Optional - just for checking the TF version & GPU.\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPUs available:\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC6Pl10BUjb6"
      },
      "source": [
        "# Dataset collecting Section"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important‼️**  \n",
        "Upload your dataset (images.zip file) before running next cell.\n",
        "  The dataset orientation should be like :\n",
        "```\n",
        "images.zip\n",
        "    |\n",
        "  images\n",
        "    |----- Img01.jpg\n",
        "    |----- Img01.xml\n",
        "    |----- Img02.jpg\n",
        "    |----- Img02.xml\n",
        "    :\n",
        "    :.. .\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "rnY1hsgXA4FC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs_FdMZ6VNqu"
      },
      "outputs": [],
      "source": [
        "# Make directories for different datasets.\n",
        "!mkdir /content/images\n",
        "!unzip -q images.zip -d /content/images/all\n",
        "!mkdir /content/images/train; mkdir /content/images/validation; mkdir /content/images/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eySns0eTXf3K"
      },
      "outputs": [],
      "source": [
        "# Download & run the data splitting script\n",
        "!wget https://raw.githubusercontent.com/superguine/ODProject/main/split.py\n",
        "!python split.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9zrYx7FXs15"
      },
      "outputs": [],
      "source": [
        "# Download data conversion scripts\n",
        "! wget https://raw.githubusercontent.com/superguine/ODProject/main/csv_convert.py\n",
        "! wget https://raw.githubusercontent.com/superguine/ODProject/main/tfrecord.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next cell creates a  \"labelmap.txt\" file with a list of classes the object detection model will detect.You have to write each class in a new line (between '**cat <<EOF >> /content /labelmap.txt** '  & ' **EOF** ' )which are present in the dataset.  \n",
        "> Suppose your classes are: ***Dry, EatenByInsects, Fresh, PartiallyDry, Wrinkled***.\n",
        "then the cell should look like:  \n",
        "```\n",
        "%%bash\n",
        "cat <<EOF >> /content/labelmap.txt\n",
        "Dry\n",
        "EatenByInsects\n",
        "Fresh\n",
        "PartiallyDry\n",
        "Wrinkled\n",
        "EOF\n",
        "```"
      ],
      "metadata": {
        "id": "SHY7Bq9k0--8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8RHLxvZXl4T"
      },
      "outputs": [],
      "source": [
        "# Create \"labelmap.txt\"\n",
        "%%bash\n",
        "cat <<EOF >> /content/labelmap.txt\n",
        "Dry\n",
        "EatenByInsects\n",
        "Fresh\n",
        "PartiallyDry\n",
        "Wrinkled\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOPnrZyHYDmH"
      },
      "outputs": [],
      "source": [
        "# Create CSV data files and TFRecord files\n",
        "!python3 csv_convert.py\n",
        "!python3 tfrecord.py --csv_input=images/train_labels.csv --labelmap=labelmap.txt --image_dir=images/train --output_path=train.tfrecord\n",
        "!python3 tfrecord.py --csv_input=images/validation_labels.csv --labelmap=labelmap.txt --image_dir=images/validation --output_path=val.tfrecord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJYCvfd-YIn5"
      },
      "outputs": [],
      "source": [
        "# Store the locations of the TFRecord and labelmap files as variables\n",
        "train_record_fname = '/content/train.tfrecord'\n",
        "val_record_fname = '/content/val.tfrecord'\n",
        "label_map_pbtxt_fname = '/content/labelmap.pbtxt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbR67Mvi_2iP"
      },
      "source": [
        "# Training Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKPqtL95LURN"
      },
      "source": [
        "#### Configuring for training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 🔵 **Note (April 2025):**  \n",
        "> The `efficientdet-d0` model currently cannot be trained due to conflicts in its configuration file.  \n",
        "> ⚠️ It's recommended to **avoid selecting this specific model** until the issue is resolved."
      ],
      "metadata": {
        "id": "3HQ7seIl3bWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdCNain2AMWx"
      },
      "outputs": [],
      "source": [
        "# Change the chosen_model variable to deploy different models available in the TF2 object detection zoo\n",
        "chosen_model = 'ssd-mobilenet-v2'\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'ssd-mobilenet-v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    'efficientdet-d0': {\n",
        "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
        "    },\n",
        "    'ssd-mobilenet-v2-fpnlite-320': {\n",
        "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "}\n",
        "\n",
        "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
        "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
        "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74wta-pzAs_K"
      },
      "source": [
        "In nex cell we'll download the pretrained model file and configuration file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I06qvPpxAr8l"
      },
      "outputs": [],
      "source": [
        "# Create \"mymodel\" folder for holding pre-trained weights and configuration files\n",
        "%mkdir /content/models/mymodel/\n",
        "%cd /content/models/mymodel/\n",
        "\n",
        "# Download pre-trained model weights\n",
        "import tarfile\n",
        "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
        "!wget {download_tar}\n",
        "tar = tarfile.open(pretrained_checkpoint)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "# Download training configuration file for model\n",
        "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
        "!wget {download_config}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvAgqNkkBO33"
      },
      "outputs": [],
      "source": [
        "# Set training parameters for the model\n",
        "num_steps = 50000\n",
        "\n",
        "if chosen_model == 'efficientdet-d0':\n",
        "  batch_size = 4\n",
        "else:\n",
        "  batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBpXpnKsBRQV"
      },
      "outputs": [],
      "source": [
        "# Set file locations and get number of classes for config file\n",
        "pipeline_fname = '/content/models/mymodel/' + base_pipeline_file\n",
        "fine_tune_checkpoint = '/content/models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    import tensorflow as tf  # Import TensorFlow explicitly\n",
        "\n",
        "    with tf.io.gfile.GFile(pbtxt_fname, 'r') as fid:  # Use tf.io.gfile.GFile\n",
        "        label_map_string = fid.read()\n",
        "\n",
        "    from object_detection.protos import string_int_label_map_pb2\n",
        "    from google.protobuf import text_format\n",
        "\n",
        "    label_map = string_int_label_map_pb2.StringIntLabelMap()\n",
        "    text_format.Merge(label_map_string, label_map)\n",
        "\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True\n",
        "    )\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "    return len(category_index.keys())\n",
        "\n",
        "# Call the function\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "print('Total classes:', num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNvsIfycBUnL"
      },
      "outputs": [],
      "source": [
        "# Create custom configuration file by writing the dataset, model checkpoint, and training parameters into the base pipeline file\n",
        "import re\n",
        "\n",
        "%cd /content/models/mymodel\n",
        "print('writing custom configuration file')\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open('pipeline_file.config', 'w') as f:\n",
        "\n",
        "    # Set fine_tune_checkpoint path\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "\n",
        "    # Set tfrecord files for train and test datasets\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n",
        "\n",
        "    # Set label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set batch_size\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "\n",
        "    # Set number of classes num_classes\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "\n",
        "    # Change fine-tune checkpoint type from \"classification\" to \"detection\"\n",
        "    s = re.sub(\n",
        "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "\n",
        "    # If using ssd-mobilenet-v2, reduce learning rate (because it's too high in the default config file)\n",
        "    if chosen_model == 'ssd-mobilenet-v2':\n",
        "      s = re.sub('learning_rate_base: .8',\n",
        "                 'learning_rate_base: .08', s)\n",
        "\n",
        "      s = re.sub('warmup_learning_rate: 0.13333',\n",
        "                 'warmup_learning_rate: .026666', s)\n",
        "\n",
        "    # If using efficientdet-d0, use fixed_shape_resizer instead of keep_aspect_ratio_resizer (because it isn't supported by TFLite)\n",
        "    if chosen_model == 'efficientdet-d0':\n",
        "      #s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n",
        "      s = re.sub('pad_to_max_dimension: true', '', s)\n",
        "      s = re.sub('min_dimension', 'height', s)\n",
        "      s = re.sub('max_dimension', 'width', s)\n",
        "\n",
        "    f.write(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional - at this point, you can see the custom configuration file by just clicking this: ` /content/models/mymodel/pipeline_file.config  `."
      ],
      "metadata": {
        "id": "4hGD9Ddd5lml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHnpZzDOBnfk"
      },
      "outputs": [],
      "source": [
        "# Set the path to the custom config file and the directory to store training checkpoints in\n",
        "pipeline_file = '/content/models/mymodel/pipeline_file.config'\n",
        "model_dir = '/content/training/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify conflicting files.\n",
        "import os\n",
        "\n",
        "# Define the file path\n",
        "file_path = \"/usr/local/lib/python3.11/dist-packages/tf_slim/data/tfexample_decoder.py\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Error: {file_path} not found.\")\n",
        "else:\n",
        "    # Read the file contents\n",
        "    with open(file_path, \"r\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Apply fixes\n",
        "    content = content.replace(\n",
        "        \"from tensorflow.python.ops import control_flow_ops\", \"import tensorflow as tf\"\n",
        "    )\n",
        "    content = content.replace(\"control_flow_ops.case\", \"tf.case\")\n",
        "    content = content.replace(\n",
        "        \"tf.case(pred_fn_pairs, default=check_jpeg, exclusive=True)\",\n",
        "        \"tf.switch_case(branch_index=tf.constant(0), branch_fns={0: lambda: check_jpeg()})\"\n",
        "    )\n",
        "    content = content.replace(\"control_flow_ops.cond\", \"tf.cond\")\n",
        "\n",
        "    # Write the modified content back to the file\n",
        "    with open(file_path, \"w\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "    print(\" Modifications applied successfully.\")\n"
      ],
      "metadata": {
        "id": "x9f7GhQb7RU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTqe0q4gCEEo"
      },
      "source": [
        "In the next cell, we'll launch [TensorBoard](https://www.tensorflow.org/tensorboard) to monitor training progress.\n",
        "\n",
        "> 🔵 **Note:**  \n",
        "> TensorBoard **won’t display anything yet**, since training hasn’t started.  \n",
        "> Once training begins, you'll start seeing logs like the example below in the output:\n",
        "```\n",
        "INFO:tensorflow:Step 22000 per-step time 0.151s\n",
        "I0412 07:38:16.052670 15402206208000 model_lib_v2.py:705] Step 22000 per-step time 0.151s\n",
        "INFO:tensorflow:{'Loss/classification_loss': 0.2932836,\n",
        " 'Loss/localization_loss': 0.17571348,\n",
        " 'Loss/regularization_loss': 0.21325982,\n",
        " 'Loss/total_loss': 0.68225694,\n",
        " 'learning_rate': 0.05035276}\n",
        "I0412 07:38:16.052977 15402206208000 model_lib_v2.py:708] {'Loss/classification_loss': 0.2932836,\n",
        " 'Loss/localization_loss': 0.17571348,\n",
        " 'Loss/regularization_loss': 0.21325982,\n",
        " 'Loss/total_loss': 0.68225694,\n",
        " 'learning_rate': 0.05035276}\n",
        " ```\n",
        "  Then come back and click the refresh button to see the model's overall loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6ZlLtEGB4aE"
      },
      "outputs": [],
      "source": [
        "#launching Tensorboard session\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/training/train'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgJTmIk3JlwW"
      },
      "source": [
        "#### Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHcaZVlDECqW"
      },
      "outputs": [],
      "source": [
        "# Run training!\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_file} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjniVdmh1_8"
      },
      "source": [
        "# Model exportation & testing Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-xZ47AaJ_TH"
      },
      "source": [
        "#### Export model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-ZrCcPNHUGk"
      },
      "outputs": [],
      "source": [
        "# Make a directory to store the trained TensorFlow model\n",
        "!mkdir /content/custom_model\n",
        "output_directory = '/content/custom_model'\n",
        "\n",
        "# Path to training directory (the conversion script automatically chooses the highest checkpoint file)\n",
        "last_model_path = '/content/training'\n",
        "\n",
        "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --trained_checkpoint_dir {last_model_path} \\\n",
        "    --pipeline_config_path {pipeline_file} \\\n",
        "    --output_directory {output_directory}\n",
        "\n",
        "print(\"\\n Done exporting \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJGz6PTxJfQn"
      },
      "source": [
        "####Inference test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LorN9Ma6Hx8i"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "### Define function for inferencing with TensorFlow model and displaying results\n",
        "\n",
        "def tf_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n",
        "\n",
        "    # Grab filenames of all images in test folder\n",
        "    images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
        "\n",
        "    # Load the label map into memory\n",
        "    with open(lblpath, 'r') as f:\n",
        "        labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    # Load the TensorFlow model\n",
        "    model = tf.saved_model.load(modelpath)\n",
        "    infer = model.signatures['serving_default']\n",
        "\n",
        "    # Randomly select test images\n",
        "    images_to_test = random.sample(images, num_test_images)\n",
        "\n",
        "    # Loop over every image and perform detection\n",
        "    for image_path in images_to_test:\n",
        "\n",
        "        # Load image and resize to expected shape [1xHxWx3]\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        imH, imW, _ = image.shape\n",
        "        input_tensor = tf.convert_to_tensor(image_rgb)\n",
        "        input_tensor = input_tensor[tf.newaxis, ...]\n",
        "\n",
        "        # Perform the actual detection by running the model with the image as input\n",
        "        detections = infer(input_tensor)\n",
        "\n",
        "        # Retrieve detection results\n",
        "        boxes = detections['detection_boxes'][0].numpy() # Bounding box coordinates of detected objects\n",
        "        classes = detections['detection_classes'][0].numpy().astype(np.int32) # Class index of detected objects\n",
        "        scores = detections['detection_scores'][0].numpy() # Confidence of detected objects\n",
        "\n",
        "        detection_results = []\n",
        "\n",
        "        # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
        "        for i in range(len(scores)):\n",
        "            if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
        "\n",
        "                # Get bounding box coordinates and draw box\n",
        "                # Coordinates are normalized, so we multiply them by the image dimensions to get pixel values\n",
        "                ymin = int(max(1, (boxes[i][0] * imH)))\n",
        "                xmin = int(max(1, (boxes[i][1] * imW)))\n",
        "                ymax = int(min(imH, (boxes[i][2] * imH)))\n",
        "                xmax = int(min(imW, (boxes[i][3] * imW)))\n",
        "\n",
        "                cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n",
        "\n",
        "                # Draw label\n",
        "                object_name = labels[classes[i] - 1] # Adjust for zero-indexing in labels\n",
        "                label = '%s: %d%%' % (object_name, int(scores[i] * 100)) # Example: 'person: 72%'\n",
        "                labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
        "                label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
        "                cv2.rectangle(image, (xmin, label_ymin - labelSize[1] - 10),\n",
        "                              (xmin + labelSize[0], label_ymin + baseLine - 10),\n",
        "                              (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
        "                cv2.putText(image, label, (xmin, label_ymin - 7),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
        "\n",
        "                detection_results.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # Display or save the image with detections\n",
        "        if not txt_only: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            plt.figure(figsize=(12, 16))\n",
        "            plt.imshow(image)\n",
        "            plt.show()\n",
        "\n",
        "        # Save detection results in .txt files (for calculating mAP)\n",
        "        else:\n",
        "            # Get filenames and paths\n",
        "            image_fn = os.path.basename(image_path)\n",
        "            base_fn, ext = os.path.splitext(image_fn)\n",
        "            txt_result_fn = base_fn + '.txt'\n",
        "            txt_savepath = os.path.join(savepath, txt_result_fn)\n",
        "\n",
        "            # Write results to text file\n",
        "            # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
        "            with open(txt_savepath, 'w') as f:\n",
        "                for detection in detection_results:\n",
        "                    f.write('%s %.4f %d %d %d %d\\n' %\n",
        "                            (detection[0], detection[1], detection[2],\n",
        "                             detection[3], detection[4], detection[5]))\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkde9Cz5Jz_d"
      },
      "outputs": [],
      "source": [
        "# Set up variables for running user's model\n",
        "PATH_TO_IMAGES = '/content/images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL = '/content/custom_model/saved_model'   # Path to the SavedModel directory\n",
        "PATH_TO_LABELS = '/content/labelmap.txt'   # Path to labelmap.txt file\n",
        "min_conf_threshold = 0.5   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
        "images_to_test = 10   # Number of images to run detection on\n",
        "\n",
        "# Run inferencing function!\n",
        "tf_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjCiBKdxKQUd"
      },
      "source": [
        "#### Calculate mAP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a visual sense of how our model performs on test images.Now wer'e going to calculate the accuracy of our model through a popular method, which is \"mean average precision\" (mAP). We'll use the mAP calculator tool at [here](https://github.com/Cartucho/mAP) to determine our model's mAP score.\n",
        "\n",
        "First, we need to clone the repository and remove its existing example data. We'll also download a script I wrote for interfacing with the calculator."
      ],
      "metadata": {
        "id": "N_ckqeWqBF0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/Cartucho/mAP /content/mAP\n",
        "cd /content/mAP\n",
        "rm input/detection-results/*\n",
        "rm input/ground-truth/*\n",
        "rm input/images-optional/*\n",
        "wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py"
      ],
      "metadata": {
        "id": "JlWarXEZDUqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll copy the images and annotation data from the **test** folder to the appropriate folders inside the cloned repository. These will be used as the \"ground truth data\" that our model's detection results will be compared to.\n"
      ],
      "metadata": {
        "id": "qn22nGGqH5T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/images/test/* /content/mAP/input/images-optional # Copy images and xml files\n",
        "!mv /content/mAP/input/images-optional/*.xml /content/mAP/input/ground-truth/  # Move xml files to the appropriate folder"
      ],
      "metadata": {
        "id": "5szFfVxwI3wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The calculator tool expects annotation data in a format that's different from the Pascal VOC .xml file format we're using. Fortunately, it provides an easy script, `convert_gt_xml.py`, for converting to the expected .txt format.\n",
        "\n"
      ],
      "metadata": {
        "id": "u6aro817DGzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert necessary files from .xml to .txt\n",
        "!python /content/mAP/scripts/extra/convert_gt_xml.py"
      ],
      "metadata": {
        "id": "qdjtOUDnK2AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up variables for running inference, this time to get detection results saved as .txt files\n",
        "PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL='/content/custom_model/saved_model'   # Path to .tflite model file\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n",
        "PATH_TO_RESULTS='/content/mAP/input/detection-results' # Folder to save detection results in\n",
        "min_conf_threshold=0.1   # Confidence threshold\n",
        "\n",
        "# Use all the images in the test folder\n",
        "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
        "images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n",
        "\n",
        "# Tell function to just save results and not display images\n",
        "txt_only = True\n",
        "\n",
        "# Run inferencing function!\n",
        "print('Starting inference on %d images...' % images_to_test)\n",
        "tf_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
        "print('Finished inferencing!')"
      ],
      "metadata": {
        "id": "szzHFAhsMNFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mAP & store the score in 'mAP_output.txt' file\n",
        "%cd /content/mAP\n",
        "!python calculate_map_cartucho.py --labels=/content/labelmap.txt > /content/mAP_output.txt\n",
        "\n",
        "# Show the scores in cell output.\n",
        "!cat /content/mAP_output.txt"
      ],
      "metadata": {
        "id": "3DkjpIBARTQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Downloading Section"
      ],
      "metadata": {
        "id": "FFTaTsDbFWnb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui-UlVw_K5bW"
      },
      "source": [
        "#### Download the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright! Now we’ll gather all the necessary components of our trained model and then compress them into a `.zip` file for easy storage or sharing."
      ],
      "metadata": {
        "id": "eDVrb9rXABLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtjjo4AXKZG-"
      },
      "outputs": [],
      "source": [
        "# Move labelmap and pipeline config files into TF model folder and zip it up\n",
        "!cp /content/labelmap.txt /content/custom_model\n",
        "!cp /content/labelmap.pbtxt /content/custom_model\n",
        "!cp /content/models/mymodel/pipeline_file.config /content/custom_model\n",
        "!cp /content/mAP_output.txt /content/custom_model\n",
        "\n",
        "# Compress all.\n",
        "%cd /content\n",
        "!zip -r custom_model.zip custom_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model is now ready for download and deployment."
      ],
      "metadata": {
        "id": "9V2zcJTKBNDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8aJyqc8LKCh"
      },
      "outputs": [],
      "source": [
        "# Dounload the model\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/custom_model.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional:** You can download the test and validation datasets used during evaluation.  \n",
        "_It's always a good idea to archive these datasets for future improvement and tracking._\n",
        "\n",
        "**⚠️Important :**  To download both files google will ask for downloading multiple files. Please click **Allow**"
      ],
      "metadata": {
        "id": "VFkgwlY0B16f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download test & validation dataset.\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the test folder\n",
        "shutil.make_archive('/content/test_images', 'zip', '/content/images/test')\n",
        "# Download the zip\n",
        "files.download('/content/test_images.zip')\n",
        "\n",
        "# Zip the validation folder\n",
        "shutil.make_archive('/content/validation_images', 'zip', '/content/images/validation')\n",
        "# Download the zip\n",
        "files.download('/content/validation_images.zip')"
      ],
      "metadata": {
        "id": "AHWrim9LBsmb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}